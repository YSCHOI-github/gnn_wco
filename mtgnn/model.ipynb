{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sacred-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "miniature-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emotional-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-32GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "permanent-situation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.0000, 0.3333, 0.3333],\n",
       "        [0.0000, 0.3333, 0.3333, 0.3333],\n",
       "        [0.0000, 0.0000, 0.6667, 0.3333],\n",
       "        [0.0000, 0.0000, 0.3333, 0.6667]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GraphGenerator(nn.Module):\n",
    "    def __init__(self, k, in_channels, dim, alpha=3.0):\n",
    "        \n",
    "        '''\n",
    "        dim - dimension of the node embeddings \n",
    "        alpha - control saturation of the tanh\n",
    "        \n",
    "        produces nonsymmetric adjacency matrix\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        super(GraphGenerator, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.lin1 = nn.Linear(in_channels,dim)\n",
    "\n",
    "\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        nodevec1 = x\n",
    "        nodevec2 = nodevec1\n",
    "        \n",
    "#         print(nodevec1.shape)\n",
    "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self.alpha*self.lin1(nodevec2))\n",
    "        \n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))\n",
    "        adj = F.relu(torch.tanh(self.alpha*a))\n",
    "        \n",
    "        mask = torch.zeros(x.size(0), x.size(0)).type_as(x)\n",
    "        mask.fill_(float('0'))\n",
    "        \n",
    "        s1,t1 = adj.topk(self.k,1)\n",
    "        \n",
    "        mask.scatter_(1,t1,s1.fill_(1))\n",
    "        \n",
    "        adj = adj*mask\n",
    "        \n",
    "        adj = self.normalize(adj)\n",
    "        \n",
    "        return adj\n",
    "    \n",
    "    def normalize(self, adj):\n",
    "        \n",
    "        adj = adj + torch.eye(adj.size(0)).type_as(adj)\n",
    "        d = adj.sum(1)\n",
    "        dv = d\n",
    "        a = adj / dv.view(-1, 1)\n",
    "        \n",
    "        return a\n",
    "k = 2\n",
    "dim = 2048\n",
    "alpha = 3\n",
    "static_feat = torch.rand(4,27)\n",
    "gu = GraphGenerator(k, 27, dim)\n",
    "\n",
    "x = torch.randint(0,10000, (4,1))\n",
    "\n",
    "adj = gu(static_feat)\n",
    "adj\n",
    "# y - torch.transpose(y,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metropolitan-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixHop(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, powers):\n",
    "        super(MixHop, self).__init__()\n",
    "        \n",
    "        self.w_list = nn.ModuleList()\n",
    "        \n",
    "        for i in range(powers):\n",
    "            \n",
    "            lin = nn.Linear(in_channels, out_channels)\n",
    "            self.w_list.append(lin)\n",
    "            \n",
    "            \n",
    "#     def init_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Initializing weights.\n",
    "#         \"\"\"\n",
    "#         torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "#         torch.nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, norm_adj, x):\n",
    "        \n",
    "        adj_power = torch.eye(norm_adj.size(0)).type_as(x)\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        for lin in self.w_list:\n",
    "            \n",
    "            prod = F.relu(torch.mm(adj_power, lin(x)))\n",
    "\n",
    "            X.append(prod)\n",
    "            \n",
    "            adj_power = torch.mm(adj_power, norm_adj)\n",
    "        \n",
    "        x = torch.stack(X, dim=1)\n",
    "\n",
    "        x = torch.sum(x, dim=1)\n",
    "        \n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dimensional-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, norm_adj, x):\n",
    "            \n",
    "        x = F.relu(torch.mm(norm_adj, self.W(x)))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fixed-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_MixHop(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, dim,  layers, powers, k):\n",
    "        super(GNN_MixHop, self).__init__()\n",
    "        \n",
    "        self.mh_layers = nn.ModuleList()\n",
    "        \n",
    "        self.graph_gen = GraphGenerator(k, in_channels, dim)\n",
    "        \n",
    "        in_channels = in_channels\n",
    "        \n",
    "        for l in layers:\n",
    "            mh = MixHop(in_channels, l, powers)\n",
    "            in_channels = l\n",
    "            \n",
    "            self.mh_layers.append(mh)\n",
    "                \n",
    "        self.linear = nn.Linear(layers[len(layers) - 1],1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        adj = self.graph_gen(x)\n",
    "\n",
    "        for mh in self.mh_layers:\n",
    "            x = mh(adj, x)\n",
    "            x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        \n",
    "        x = self.linear(x)\n",
    "#         print(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nasty-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_GCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, dim,  layers, powers, k):\n",
    "        super(GNN_GCN, self).__init__()\n",
    "        \n",
    "        self.mh_layers = nn.ModuleList()\n",
    "        \n",
    "        self.graph_gen = GraphGenerator(k, in_channels, dim)\n",
    "        \n",
    "        in_channels = in_channels\n",
    "        \n",
    "        for l in layers:\n",
    "            gcn = GCN(in_channels, l)\n",
    "            in_channels = l\n",
    "            \n",
    "            self.mh_layers.append(gcn)\n",
    "                \n",
    "        self.linear = nn.Linear(layers[len(layers) - 1],1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        adj = self.graph_gen(x)\n",
    "\n",
    "        for gcn in self.mh_layers:\n",
    "            \n",
    "            x = gcn(adj, x)\n",
    "            x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        \n",
    "        x = self.linear(x)\n",
    "#         print(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "limiting-chicago",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['raw', 'xgboost_data', 'revenue'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# load preprocessed data\n",
    "with open(\"Dual-Attentive-Tree-aware-Embedding/processed_data.pickle\",\"rb\") as f :\n",
    "    processed_data = pickle.load(f)\n",
    "print(processed_data.keys())\n",
    "\n",
    "xgb_trainx = processed_data[\"xgboost_data\"][\"train_x\"]\n",
    "xgb_trainy = processed_data[\"xgboost_data\"][\"train_y\"]\n",
    "xgb_validx = processed_data[\"xgboost_data\"][\"valid_x\"]\n",
    "xgb_validy = processed_data[\"xgboost_data\"][\"valid_y\"]\n",
    "xgb_testx = processed_data[\"xgboost_data\"][\"test_x\"]\n",
    "xgb_testy = processed_data[\"xgboost_data\"][\"test_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "every-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, i):\n",
    "    x_cut = x[:,:i]\n",
    "    x_cut = (x_cut - x_cut.mean(0)) / x_cut.std(0)\n",
    "    \n",
    "    x[:, :i] = x_cut\n",
    "    \n",
    "    return x\n",
    "\n",
    "xgb_trainx = normalize(xgb_trainx, 15)\n",
    "xgb_validx = normalize(xgb_validx, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "neutral-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23068742,  0.0450329 , -0.18982874, -0.3068576 , -0.15670664,\n",
       "        -0.05921159, -0.09469758, -0.17137938,  0.43892181,  0.43892254,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.24384909, -0.29701643, -0.18330379, -0.29769346, -0.1578157 ,\n",
       "        -0.06284325, -0.21135304, -0.18821133,  0.43857269,  0.43857189,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.043825  , -0.26290795, -0.17947537, -0.30031178, -0.15259673,\n",
       "        -0.05833124, -0.22296814, -0.1872739 ,  0.43857698,  0.43857712,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.2491878 , -0.16075787, -0.19128329, -0.3138398 , -0.13960439,\n",
       "        -0.05805424, -0.07647669,  0.05995672,  0.43857698,  0.43857712,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.3030786 , -0.25611255, -0.19069737,  0.21986236, -0.15780515,\n",
       "         0.07245682, -0.22456849, -0.18864042,  0.43861927,  0.43861899,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.43565204, -0.21244668,  0.14829666, -0.3138398 ,  0.97119935,\n",
       "        -0.06281563, -0.22412368, -0.02099989,  0.43857269,  0.43857189,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.25351767, -0.29903313, -0.19068981, -0.3138398 , -0.14662741,\n",
       "        -0.06170596, -0.19481708, -0.15661422,  0.43857698,  0.43857712,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.1456618 , -0.21051767, -0.1894036 , -0.28307447, -0.15686548,\n",
       "        -0.05002774, -0.21557041, -0.18747887,  0.32268259,  0.3226833 ,\n",
       "         0.32243391,  0.28382599, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.16030674, -0.30468864, -0.1910923 , -0.30838496, -0.15191963,\n",
       "        -0.02749097, -0.22418901, -0.18778835,  0.43857698,  0.43857712,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.23271811, -0.1563299 ,  0.37255434, -0.1966697 , -0.15809463,\n",
       "        -0.06339739, -0.16283667, -0.18820596,  0.43857698,  0.43857712,\n",
       "         0.43861831,  0.44075298, -1.72084165, -1.62464043, -2.37415772,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_validx[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "maritime-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "         \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.X[idx,:]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 32\n",
    "\n",
    "train_dataset = MTGNNDataset(xgb_trainx, xgb_trainy)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=True, \n",
    "                          drop_last=True)\n",
    "\n",
    "val_dataset = MTGNNDataset(xgb_validx, xgb_validy)\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=True, \n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prostate-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, in_channels, dim,  layers, powers, k):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.gnn = GNN_MixHop(in_channels, dim, layers, powers, k)\n",
    "        self.lr = 1e-3\n",
    "        self.l2 = 1e-4\n",
    "    \n",
    "    def _weight_init(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.kaiming_normal_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.gnn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "      \n",
    "        X, y = batch        \n",
    "        y_hat = self.forward(X)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten())\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "         \n",
    "        X, y = batch\n",
    "     \n",
    "        y_hat = self.forward(X)\n",
    "                \n",
    "        loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten())\n",
    "                   \n",
    "        threshold = 0.4\n",
    "        y_hat[y_hat >= threshold] = 1\n",
    "        y_hat[y_hat < threshold] = 0\n",
    "\n",
    "        \n",
    "        return {'val_loss': loss, \"preds\": y_hat, \"targets\": y}\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        loss = torch.mean(torch.stack([x['val_loss'] for x in val_step_outputs])).detach().cpu()\n",
    "        \n",
    "        for pred in val_step_outputs:\n",
    "                \n",
    "            y_hat = pred[\"preds\"]\n",
    "            y = pred[\"targets\"]\n",
    "                                \n",
    "            y_true.extend(y.detach().cpu().numpy().tolist())\n",
    "            y_pred.extend(y_hat.detach().cpu().numpy().tolist())\n",
    "        \n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log(\"f1\", f1, prog_bar=True)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "#         optimizer = RangerLars(self.parameters(), lr=self.lr, weight_decay=self.l2)\n",
    "#         scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.99)\n",
    "#         return [optimizer], [scheduler]\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2)\n",
    "    \n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "resistant-miracle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | gnn  | GNN_MixHop | 7.0 K \n",
      "------------------------------------\n",
      "7.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 K     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 234/234 [00:06<00:00, 37.30it/s, loss=0.12, v_num=57, val_loss=0.494, f1=0.487] \n",
      "Epoch 9: 100%|██████████| 234/234 [00:08<00:00, 27.51it/s, loss=0.12, v_num=57, val_loss=0.303, f1=0.503]\n",
      "Epoch 19: 100%|██████████| 234/234 [00:06<00:00, 37.69it/s, loss=0.133, v_num=57, val_loss=0.303, f1=0.503]\n",
      "Epoch 19: 100%|██████████| 234/234 [00:08<00:00, 28.52it/s, loss=0.133, v_num=57, val_loss=0.296, f1=0.505]\n",
      "Epoch 29: 100%|██████████| 234/234 [00:06<00:00, 37.82it/s, loss=0.109, v_num=57, val_loss=0.296, f1=0.505] \n",
      "Epoch 29: 100%|██████████| 234/234 [00:08<00:00, 28.34it/s, loss=0.109, v_num=57, val_loss=0.306, f1=0.506]\n",
      "Epoch 32:  59%|█████▉    | 139/234 [00:04<00:02, 33.21it/s, loss=0.105, v_num=57, val_loss=0.306, f1=0.506]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "in_channels = 27\n",
    "dim = 40\n",
    "layers = [32, 32] \n",
    "powers = 3\n",
    "k = 3\n",
    "\n",
    "model = Model(in_channels, dim, layers, powers, k)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='f1',\n",
    "    filename='MTGNN-{epoch:02d}-{f1:.4f}',\n",
    "    save_top_k=0,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    gpus=[0],\n",
    "    num_sanity_val_steps=1,\n",
    "    check_val_every_n_epoch=10,\n",
    "#     gradient_clip_val=5\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occupied-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_channels = 27\n",
    "# dim = 40\n",
    "# layers = [32, 32] \n",
    "# powers = 2\n",
    "# k = 16\n",
    "# clip = 5\n",
    "\n",
    "# device = 'cuda:0'\n",
    "\n",
    "# m = GNN_GCN(in_channels, dim, layers, powers, k)\n",
    "# m.to(device)\n",
    "        \n",
    "# optimizer = torch.optim.Adam(m.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# for i in range(epochs):\n",
    "    \n",
    "#     m.train()\n",
    "#     print('train epoch ', i+1)\n",
    "#     for x, y in tqdm(train_loader):\n",
    "        \n",
    "#         x, y = x.to(device), y.to(device)\n",
    "        \n",
    "#         y_hat = m(x.to(device))\n",
    "        \n",
    "# #         print(y_hat)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten())\n",
    "#         loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(m.parameters(), clip)\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     m.eval()\n",
    "    \n",
    "#     val_loss = []\n",
    "\n",
    "#     preds = []\n",
    "#     targets = []    \n",
    "    \n",
    "#     for x, y in val_loader:\n",
    "        \n",
    "#         x, y = x.to(device), y.to(device)\n",
    "        \n",
    "#         y_hat = m(x.to(device))\n",
    "        \n",
    "#         loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten()).item()\n",
    "        \n",
    "#         val_loss.append(loss)\n",
    "        \n",
    "# #         print(y_hat.flatten())\n",
    "#         threshold = 0.3\n",
    "#         y_hat[y_hat >= threshold] = 1\n",
    "#         y_hat[y_hat < threshold] = 0\n",
    "                                        \n",
    "#         targets.extend(y.detach().cpu().numpy().tolist())\n",
    "#         preds.extend(y_hat.detach().cpu().numpy().tolist())\n",
    "        \n",
    "#     f1 = f1_score(targets,preds, average='macro')\n",
    "    \n",
    "#     val_loss = np.mean(val_loss)\n",
    "# #     print(targets, preds)\n",
    "#     print('val_loss ', val_loss)\n",
    "#     print('f1 ', f1)\n",
    "    \n",
    "    \n",
    "# # # for p in model.parameters():\n",
    "# # #     print(p.grad.norm())\n",
    "\n",
    "# # # y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-matthew",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robert3",
   "language": "python",
   "name": "robert3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
