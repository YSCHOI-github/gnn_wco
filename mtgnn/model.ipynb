{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sacred-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "miniature-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "emotional-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-32GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "permanent-situation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.0000, 0.3333, 0.3333],\n",
       "        [0.0000, 0.3333, 0.3333, 0.3333],\n",
       "        [0.0000, 0.0000, 0.6667, 0.3333],\n",
       "        [0.0000, 0.0000, 0.3333, 0.6667]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GraphGenerator(nn.Module):\n",
    "    def __init__(self, k, in_channels, dim, alpha=3.0):\n",
    "        \n",
    "        '''\n",
    "        dim - dimension of the node embeddings \n",
    "        alpha - control saturation of the tanh\n",
    "        \n",
    "        produces nonsymmetric adjacency matrix\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        super(GraphGenerator, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.lin1 = nn.Linear(in_channels,dim)\n",
    "\n",
    "\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        nodevec1 = x\n",
    "        nodevec2 = nodevec1\n",
    "        \n",
    "#         print(nodevec1.shape)\n",
    "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self.alpha*self.lin1(nodevec2))\n",
    "        \n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))\n",
    "        adj = F.relu(torch.tanh(self.alpha*a))\n",
    "        \n",
    "        mask = torch.zeros(x.size(0), x.size(0)).type_as(x)\n",
    "        mask.fill_(float('0'))\n",
    "        \n",
    "        s1,t1 = adj.topk(self.k,1)\n",
    "        \n",
    "        mask.scatter_(1,t1,s1.fill_(1))\n",
    "        \n",
    "        adj = adj*mask\n",
    "        \n",
    "        adj = self.normalize(adj)\n",
    "        \n",
    "        return adj\n",
    "    \n",
    "    def normalize(self, adj):\n",
    "        \n",
    "        adj = adj + torch.eye(adj.size(0)).type_as(adj)\n",
    "        d = adj.sum(1)\n",
    "        dv = d\n",
    "        a = adj / dv.view(-1, 1)\n",
    "        \n",
    "        return a\n",
    "k = 2\n",
    "dim = 2048\n",
    "alpha = 3\n",
    "static_feat = torch.rand(4,27)\n",
    "gu = GraphGenerator(k, 27, dim)\n",
    "\n",
    "x = torch.randint(0,10000, (4,1))\n",
    "\n",
    "adj = gu(static_feat)\n",
    "adj\n",
    "# y - torch.transpose(y,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metropolitan-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixHop(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, powers):\n",
    "        super(MixHop, self).__init__()\n",
    "        \n",
    "        self.w_list = nn.ModuleList()\n",
    "        \n",
    "        for i in range(powers):\n",
    "            \n",
    "            lin = nn.Linear(in_channels, out_channels)\n",
    "            self.w_list.append(lin)\n",
    "            \n",
    "            \n",
    "#     def init_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Initializing weights.\n",
    "#         \"\"\"\n",
    "#         torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "#         torch.nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, norm_adj, x):\n",
    "        \n",
    "        adj_power = torch.eye(norm_adj.size(0)).type_as(x)\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        for lin in self.w_list:\n",
    "            \n",
    "            prod = F.relu(torch.mm(adj_power, lin(x)))\n",
    "\n",
    "            X.append(prod)\n",
    "            \n",
    "            adj_power = torch.mm(adj_power, norm_adj)\n",
    "        \n",
    "        x = torch.stack(X, dim=1)\n",
    "\n",
    "        x = torch.sum(x, dim=1)\n",
    "        \n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dimensional-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, norm_adj, x):\n",
    "            \n",
    "        x = F.relu(torch.mm(norm_adj, self.W(x)))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fixed-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_MixHop(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, dim,  layers, powers, k, p=0.5):\n",
    "        super(GNN_MixHop, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "        \n",
    "        self.mh_layers = nn.ModuleList()\n",
    "        \n",
    "        self.graph_gen = GraphGenerator(k, in_channels, dim)\n",
    "        \n",
    "        in_channels = in_channels\n",
    "        \n",
    "        for l in layers:\n",
    "            mh = MixHop(in_channels, l, powers)\n",
    "            in_channels = l\n",
    "            \n",
    "            self.mh_layers.append(mh)\n",
    "                \n",
    "        self.linear = nn.Linear(layers[len(layers) - 1],1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        adj = self.graph_gen(x)\n",
    "\n",
    "        for mh in self.mh_layers:\n",
    "            x = mh(adj, x)\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "\n",
    "        \n",
    "        x = self.linear(x)\n",
    "#         print(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nasty-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_GCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, dim,  layers, powers, k, p=0.5):\n",
    "        super(GNN_GCN, self).__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        \n",
    "        self.mh_layers = nn.ModuleList()\n",
    "        \n",
    "        self.graph_gen = GraphGenerator(k, in_channels, dim)\n",
    "        \n",
    "        in_channels = in_channels\n",
    "        \n",
    "        for l in layers:\n",
    "            gcn = GCN(in_channels, l)\n",
    "            in_channels = l\n",
    "            \n",
    "            self.mh_layers.append(gcn)\n",
    "                \n",
    "        self.linear = nn.Linear(layers[len(layers) - 1],1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        adj = self.graph_gen(x)\n",
    "\n",
    "        for gcn in self.mh_layers:\n",
    "            \n",
    "            x = gcn(adj, x)\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "\n",
    "        \n",
    "        x = self.linear(x)\n",
    "#         print(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "limiting-chicago",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'train_y', 'valid', 'valid_y'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# load preprocessed data\n",
    "with open(\"../tdata.pickle\",\"rb\") as f :\n",
    "    processed_data = pickle.load(f)\n",
    "print(processed_data.keys())\n",
    "\n",
    "train = processed_data['train']\n",
    "train_y = processed_data['train_y']\n",
    "\n",
    "valid = processed_data['valid']\n",
    "valid_y = processed_data['valid_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "every-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, i):\n",
    "    x_cut = x[:,:i]\n",
    "    x_cut = (x_cut - x_cut.mean(0)) / x_cut.std(0)\n",
    "    \n",
    "    x[:, :i] = x_cut\n",
    "    \n",
    "    return x\n",
    "\n",
    "train = normalize(train, 15)\n",
    "valid = normalize(valid, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "neutral-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134457, 29), (134457,), (30957, 29), (30957,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape, valid_y.shape, train.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "maritime-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "         \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.X[idx,:]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 32\n",
    "\n",
    "train_dataset = MTGNNDataset(train, train_y)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=True, \n",
    "                          drop_last=True)\n",
    "\n",
    "val_dataset = MTGNNDataset(valid, valid_y)\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=True, \n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prostate-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, in_channels, dim,  layers, powers, k, p=0.5):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.gnn = GNN_MixHop(in_channels, dim, layers, powers, k, p=p)\n",
    "        self.lr = 1e-3\n",
    "        self.l2 = 1e-4\n",
    "        \n",
    "    def _weight_init(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.kaiming_normal_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.gnn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "      \n",
    "        X, y = batch        \n",
    "        y_hat = self.forward(X)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten())\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "         \n",
    "        X, y = batch\n",
    "     \n",
    "        y_hat = self.forward(X)\n",
    "                \n",
    "        loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten())\n",
    "                   \n",
    "        threshold = 0.3\n",
    "        y_hat[y_hat >= threshold] = 1\n",
    "        y_hat[y_hat < threshold] = 0\n",
    "\n",
    "        \n",
    "        return {'val_loss': loss, \"preds\": y_hat, \"targets\": y}\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        loss = torch.mean(torch.stack([x['val_loss'] for x in val_step_outputs])).detach().cpu()\n",
    "        \n",
    "        for pred in val_step_outputs:\n",
    "                \n",
    "            y_hat = pred[\"preds\"]\n",
    "            y = pred[\"targets\"]\n",
    "                                \n",
    "            y_true.extend(y.detach().cpu().numpy().tolist())\n",
    "            y_pred.extend(y_hat.detach().cpu().numpy().tolist())\n",
    "        \n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        prec = precision_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        self.log(\"f1\", f1, prog_bar=True)\n",
    "        self.log(\"prec\", prec, prog_bar=True)\n",
    "        self.log(\"recall\", recall, prog_bar=True)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        \n",
    "    \n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "resistant-miracle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | gnn  | GNN_MixHop | 11.3 K\n",
      "------------------------------------\n",
      "11.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 K    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2acef100fc20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/intern/robertkim/.conda/envs/robert3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/intern/robertkim/.conda/envs/robert3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/intern/robertkim/.conda/envs/robert3/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 241/241 [00:08<00:00, 27.34it/s, loss=0.17, v_num=14, val_loss=0.879, f1=0, prec=0, recall=0]  \n",
      "Epoch 9: 100%|██████████| 241/241 [00:25<00:00,  9.32it/s, loss=0.17, v_num=14, val_loss=0.355, f1=0.596, prec=0.634, recall=0.579]\n",
      "Epoch 19: 100%|██████████| 241/241 [00:08<00:00, 27.77it/s, loss=0.158, v_num=14, val_loss=0.355, f1=0.596, prec=0.634, recall=0.579] \n",
      "Epoch 19: 100%|██████████| 241/241 [00:25<00:00,  9.42it/s, loss=0.158, v_num=14, val_loss=0.357, f1=0.605, prec=0.64, recall=0.588] \n",
      "Epoch 29: 100%|██████████| 241/241 [00:08<00:00, 28.13it/s, loss=0.157, v_num=14, val_loss=0.357, f1=0.605, prec=0.64, recall=0.588] \n",
      "Epoch 29: 100%|██████████| 241/241 [00:27<00:00,  8.86it/s, loss=0.157, v_num=14, val_loss=0.353, f1=0.605, prec=0.645, recall=0.586]\n",
      "Epoch 39: 100%|██████████| 241/241 [00:08<00:00, 28.93it/s, loss=0.159, v_num=14, val_loss=0.353, f1=0.605, prec=0.645, recall=0.586] \n",
      "Epoch 39: 100%|██████████| 241/241 [00:25<00:00,  9.36it/s, loss=0.159, v_num=14, val_loss=0.346, f1=0.598, prec=0.646, recall=0.579]\n",
      "Epoch 49: 100%|██████████| 241/241 [00:08<00:00, 27.79it/s, loss=0.164, v_num=14, val_loss=0.346, f1=0.598, prec=0.646, recall=0.579] \n",
      "Epoch 49: 100%|██████████| 241/241 [00:23<00:00, 10.16it/s, loss=0.164, v_num=14, val_loss=0.349, f1=0.597, prec=0.648, recall=0.578]\n",
      "Epoch 59: 100%|██████████| 241/241 [00:08<00:00, 29.92it/s, loss=0.16, v_num=14, val_loss=0.349, f1=0.597, prec=0.648, recall=0.578]  \n",
      "Epoch 59: 100%|██████████| 241/241 [00:24<00:00,  9.82it/s, loss=0.16, v_num=14, val_loss=0.346, f1=0.597, prec=0.652, recall=0.577]\n",
      "Epoch 63:  57%|█████▋    | 138/241 [00:06<00:04, 21.43it/s, loss=0.147, v_num=14, val_loss=0.346, f1=0.597, prec=0.652, recall=0.577] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "in_channels = 29\n",
    "dim = 40\n",
    "layers = [32, 32] \n",
    "powers = 4\n",
    "k = 8\n",
    "p=0.2\n",
    "\n",
    "model = Model(in_channels, dim, layers, powers, k, p)\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor='f1',\n",
    "#     filename='MTGNN-{epoch:02d}-{f1:.4f}',\n",
    "#     save_top_k=0,\n",
    "#     mode='max',\n",
    "# )\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    gpus=[0],\n",
    "    num_sanity_val_steps=1,\n",
    "    check_val_every_n_epoch=10,\n",
    "#     gradient_clip_val=5\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_channels = 27\n",
    "# dim = 40\n",
    "# layers = [32, 32] \n",
    "# powers = 2\n",
    "# k = 16\n",
    "# clip = 5\n",
    "\n",
    "# device = 'cuda:0'\n",
    "\n",
    "# m = GNN_GCN(in_channels, dim, layers, powers, k)\n",
    "# m.to(device)\n",
    "        \n",
    "# optimizer = torch.optim.Adam(m.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# for i in range(epochs):\n",
    "    \n",
    "#     m.train()\n",
    "#     print('train epoch ', i+1)\n",
    "#     for x, y in tqdm(train_loader):\n",
    "        \n",
    "#         x, y = x.to(device), y.to(device)\n",
    "        \n",
    "#         y_hat = m(x.to(device))\n",
    "        \n",
    "# #         print(y_hat)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten())\n",
    "#         loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(m.parameters(), clip)\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     m.eval()\n",
    "    \n",
    "#     val_loss = []\n",
    "\n",
    "#     preds = []\n",
    "#     targets = []    \n",
    "    \n",
    "#     for x, y in val_loader:\n",
    "        \n",
    "#         x, y = x.to(device), y.to(device)\n",
    "        \n",
    "#         y_hat = m(x.to(device))\n",
    "        \n",
    "#         loss = F.binary_cross_entropy(y_hat.flatten(), y.flatten()).item()\n",
    "        \n",
    "#         val_loss.append(loss)\n",
    "        \n",
    "# #         print(y_hat.flatten())\n",
    "#         threshold = 0.3\n",
    "#         y_hat[y_hat >= threshold] = 1\n",
    "#         y_hat[y_hat < threshold] = 0\n",
    "                                        \n",
    "#         targets.extend(y.detach().cpu().numpy().tolist())\n",
    "#         preds.extend(y_hat.detach().cpu().numpy().tolist())\n",
    "        \n",
    "#     f1 = f1_score(targets,preds, average='macro')\n",
    "    \n",
    "#     val_loss = np.mean(val_loss)\n",
    "# #     print(targets, preds)\n",
    "#     print('val_loss ', val_loss)\n",
    "#     print('f1 ', f1)\n",
    "    \n",
    "    \n",
    "# # # for p in model.parameters():\n",
    "# # #     print(p.grad.norm())\n",
    "\n",
    "# # # y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-matthew",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robert3",
   "language": "python",
   "name": "robert3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
